{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from pre_processing.ipynb\n",
      "Importing Jupyter notebook from feature_generation_reliable.ipynb\n",
      "Importing Jupyter notebook from feature_generation_dangerous.ipynb\n",
      "Importing Jupyter notebook from feature_selection.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "\n",
    "import pre_processing\n",
    "import feature_selection\n",
    "\n",
    "import feature_generation_dangerous\n",
    "import feature_generation_reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escribir_respuesta(ids,predicciones):\n",
    "    with open(\"respuesta.csv\",'w') as archivo:\n",
    "        archivo.write(\"id,target\\n\")\n",
    "        for i in range(len(ids)):\n",
    "            linea = f\"{int(ids[i])},{predicciones[i]}\"\n",
    "            archivo.write(f\"{linea}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA: Convertir el problema en un problema de clasificacion\n",
    "Tras haber trabajado muchos dias con el dataframe original, mandando directamente a entrenar a un modelo de regresion para intentar predecir los precios, hemos llegado a obtener un MAE de 472k, que nos parece muy alto. Una idea que entonces surge, es ver donde estamos fallando por mucho.\n",
    "\n",
    "Tras realizar el **analisis de error correspondiente**, hemos podido observar que nuestro modelo de regresion esta fallando por mucho en los outliers, propiedades que generan un error absoluto de 10 millones o mas. Esto se debe a datos que no tienen sentido. Para encarar este problema, proponemos la siguiente solucion:\n",
    "\n",
    "1. Antes de aplicar modelos de regresion, entrenaremos un **modelo de clasificacion** cuyo objetivo sera determinar si el dato es o no, un outlier. El criterio para definir si un dato es o no un outlier sera especificado mas adelante. Ahora, si logramos una buena precision en este modelo, tendremos nuestro dataset original dividido en dos sub-datasets: uno con datos \"confiables\", y otro con datos feos.\n",
    "2. Una vez que tenemos nuestros dos sub-datasets, aplicaremos el proceso que venimos aplicando al dataset original hasta ahora a cada uno de estos, generando features para cada uno de manera independiente y tratando el problema como dos sub problemas. La idea es que el modelo que trata con los datos confiables no se \"maree\" con los outliers, e intentar predecir estos con un modelo que este entrenado para tal fin.\n",
    "3. Finalmente, predecimos los precios de las propiedades en test siguiendo este procedimiento y unimos todas las predicciones para dar una prediccion final.\n",
    "<hr>\n",
    "\n",
    "### IMPORTANTE:\n",
    "**NO** es necesario **correr todo el notebook para poder entrenar**. Se pueden **ejecutar por pasos**, ya que cada paso termina guardando los resultados obtenidos.\n",
    "<hr>\n",
    "\n",
    "# Paso 1: Entrenar modelo de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = pre_processing.load_featured_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_selection.get_selected_dataframe(train)\n",
    "test = feature_selection.get_selected_dataframe(test, precio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestro criterio para detectar un outlier sera que su precio sea mayor al precio promedio + el desvio.\n",
    "mean = train['precio'].describe()[1]\n",
    "std = train['precio'].describe()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_std(x, sup):\n",
    "    if (x<sup):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train['precio'].map(lambda x: bin_std(x, mean+std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    206838\n",
       "0     33162\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que con este criterio, tenemos 33k de datos \"outliers\" o \"con precios muy altos\", que pueden confundir\n",
    "# al modelo de regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo de clasificacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = train.drop('precio', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_2.drop('target', axis=1)\n",
    "Y = train_2['target']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = {'objective': 'binary',\n",
    "            'metric':'auc',\n",
    "            'num_leaves': 75,\n",
    "            'max_depth': 7,\n",
    "            'min_split_gain': 0.01,\n",
    "            'min_child_weight': 5.00001,\n",
    "            'learning_rate': 0.05,\n",
    "            'lambda_l2': 0,\n",
    "            'feature_fraction': 0.7000000000000001,\n",
    "            'bagging_fraction': 1.0,\n",
    "            'n_estimators': 99999,\n",
    "            'early_stopping_round': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's auc: 0.978176\n",
      "[1000]\tvalid_0's auc: 0.978976\n",
      "[1500]\tvalid_0's auc: 0.979177\n",
      "Early stopping, best iteration is:\n",
      "[1468]\tvalid_0's auc: 0.979189\n"
     ]
    }
   ],
   "source": [
    "d_train = lgb.Dataset(X_train.values, label=Y_train.values)\n",
    "d_valid = lgb.Dataset(X_val.values, label=Y_val.values)\n",
    "watchlist = [d_valid]\n",
    "reg = lgb.train(lightgbm, d_train, valid_sets=watchlist, verbose_eval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos nuestro clasificador listo para separar.\n",
    "Vemos que hemos obtenido un **AUC de 0.979189**, es decir que el modelo sera capaz de detectar outliers con una precision altisima. Esto es importante ya que el precio final predecido dependera muchisimo de esta separacion temprana.\n",
    "\n",
    "Vamos entonces a **clasificar los datasets originales** en dos datasets y guardarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = reg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.vectorize(lambda x: 1 if (x>0.5) else 0)\n",
    "test_predicted = f(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    52355\n",
       "0     7645\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    206838\n",
       "0     33162\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora lo que queremos es separar en dos el problema: MODELO para los datos confiables, y otro modelo\n",
    "# para los datos no confiables, y luego juntar todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outliers = train[['target']]\n",
    "test_outliers = test[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = train_outliers.append(test_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.to_csv('data/outliers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el modelo para los datos confiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = pre_processing.load_featured_reliable_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = feature_selection.get_selected_dataframe_reliable(train)\n",
    "test = feature_selection.get_selected_dataframe_reliable(test, precio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo:\n",
    "X = train.drop('precio', axis=1)\n",
    "Y = train['precio']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'max_depth': 14,\n",
    "    'num_leaves': 120,\n",
    "    #'learning_rate': 0.02,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0, \n",
    "    'early_stopping_round': 100}\n",
    "n_estimators=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's l1: 305724\n",
      "[2000]\tvalid_0's l1: 300199\n",
      "[3000]\tvalid_0's l1: 297637\n",
      "[4000]\tvalid_0's l1: 296245\n",
      "[5000]\tvalid_0's l1: 295494\n",
      "[6000]\tvalid_0's l1: 295073\n",
      "Early stopping, best iteration is:\n",
      "[6351]\tvalid_0's l1: 294946\n"
     ]
    }
   ],
   "source": [
    "d_train = lgb.Dataset(X_train.values, label=Y_train.values)\n",
    "d_valid = lgb.Dataset(X_val.values, label=Y_val.values)\n",
    "watchlist = [d_valid]\n",
    "reg = lgb.train(params, d_train, n_estimators, valid_sets=watchlist, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos las predicciones\n",
    "pred_reliable = reg.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a['precio'] = pred_reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = test_a.reset_index()[['id', 'precio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>precio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>51775</td>\n",
       "      <td>1.033856e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>115253</td>\n",
       "      <td>2.287917e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>299321</td>\n",
       "      <td>1.108731e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>173570</td>\n",
       "      <td>6.314746e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>30862</td>\n",
       "      <td>1.305523e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        precio\n",
       "0   51775  1.033856e+06\n",
       "1  115253  2.287917e+06\n",
       "2  299321  1.108731e+06\n",
       "3  173570  6.314746e+05\n",
       "4   30862  1.305523e+06"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tenemos test a\n",
    "test_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el modelo para los datos no confiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = pre_processing.load_featured_dangerous_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = feature_selection.get_selected_dataframe_dangerous(train)\n",
    "test = feature_selection.get_selected_dataframe_dangerous(test, precio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo:\n",
    "X = train.drop('precio', axis=1)\n",
    "Y = train['precio']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.05, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'max_depth': 11,\n",
    "    'num_leaves': 70,\n",
    "    'learning_rate': 0.02,\n",
    "    #'learning_rate': 0.05,\n",
    "    'verbose': 0} \n",
    "    #'early_stopping_round': 100}\n",
    "n_estimators=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's l1: 1.14059e+06\n",
      "[100]\tvalid_0's l1: 1.01839e+06\n",
      "[150]\tvalid_0's l1: 966292\n",
      "[200]\tvalid_0's l1: 938944\n",
      "[250]\tvalid_0's l1: 924212\n",
      "[300]\tvalid_0's l1: 915103\n",
      "[350]\tvalid_0's l1: 909397\n",
      "[400]\tvalid_0's l1: 905823\n",
      "[450]\tvalid_0's l1: 903137\n",
      "[500]\tvalid_0's l1: 900690\n",
      "[550]\tvalid_0's l1: 897392\n",
      "[600]\tvalid_0's l1: 895082\n",
      "[650]\tvalid_0's l1: 893048\n",
      "[700]\tvalid_0's l1: 890852\n",
      "[750]\tvalid_0's l1: 888307\n",
      "[800]\tvalid_0's l1: 887165\n",
      "[850]\tvalid_0's l1: 884856\n",
      "[900]\tvalid_0's l1: 882413\n",
      "[950]\tvalid_0's l1: 880793\n",
      "[1000]\tvalid_0's l1: 878979\n",
      "[1050]\tvalid_0's l1: 876765\n",
      "[1100]\tvalid_0's l1: 875251\n",
      "[1150]\tvalid_0's l1: 874628\n",
      "[1200]\tvalid_0's l1: 873690\n",
      "[1250]\tvalid_0's l1: 872420\n",
      "[1300]\tvalid_0's l1: 871978\n",
      "[1350]\tvalid_0's l1: 871587\n",
      "[1400]\tvalid_0's l1: 871081\n",
      "[1450]\tvalid_0's l1: 870351\n",
      "[1500]\tvalid_0's l1: 869504\n",
      "[1550]\tvalid_0's l1: 869061\n",
      "[1600]\tvalid_0's l1: 868316\n",
      "[1650]\tvalid_0's l1: 867653\n",
      "[1700]\tvalid_0's l1: 866957\n",
      "[1750]\tvalid_0's l1: 866664\n",
      "[1800]\tvalid_0's l1: 866094\n",
      "[1850]\tvalid_0's l1: 865497\n",
      "[1900]\tvalid_0's l1: 865555\n",
      "[1950]\tvalid_0's l1: 864767\n",
      "[2000]\tvalid_0's l1: 863889\n",
      "[2050]\tvalid_0's l1: 863343\n",
      "[2100]\tvalid_0's l1: 862692\n",
      "[2150]\tvalid_0's l1: 862752\n",
      "[2200]\tvalid_0's l1: 862481\n",
      "[2250]\tvalid_0's l1: 861842\n",
      "[2300]\tvalid_0's l1: 861780\n",
      "[2350]\tvalid_0's l1: 861400\n",
      "[2400]\tvalid_0's l1: 861030\n",
      "[2450]\tvalid_0's l1: 861052\n",
      "[2500]\tvalid_0's l1: 860506\n",
      "[2550]\tvalid_0's l1: 860047\n",
      "[2600]\tvalid_0's l1: 860128\n",
      "[2650]\tvalid_0's l1: 859793\n",
      "[2700]\tvalid_0's l1: 859649\n",
      "[2750]\tvalid_0's l1: 859504\n",
      "[2800]\tvalid_0's l1: 858917\n",
      "[2850]\tvalid_0's l1: 859066\n",
      "[2900]\tvalid_0's l1: 859334\n",
      "[2950]\tvalid_0's l1: 859209\n",
      "[3000]\tvalid_0's l1: 859212\n",
      "[3050]\tvalid_0's l1: 859196\n",
      "[3100]\tvalid_0's l1: 859052\n",
      "[3150]\tvalid_0's l1: 858993\n",
      "[3200]\tvalid_0's l1: 859187\n",
      "[3250]\tvalid_0's l1: 859375\n",
      "[3300]\tvalid_0's l1: 859613\n",
      "[3350]\tvalid_0's l1: 859675\n",
      "[3400]\tvalid_0's l1: 859534\n",
      "[3450]\tvalid_0's l1: 859334\n",
      "[3500]\tvalid_0's l1: 859112\n",
      "[3550]\tvalid_0's l1: 859067\n",
      "[3600]\tvalid_0's l1: 858938\n",
      "[3650]\tvalid_0's l1: 858722\n",
      "[3700]\tvalid_0's l1: 859058\n",
      "[3750]\tvalid_0's l1: 858964\n",
      "[3800]\tvalid_0's l1: 858979\n",
      "[3850]\tvalid_0's l1: 859076\n",
      "[3900]\tvalid_0's l1: 859080\n",
      "[3950]\tvalid_0's l1: 858978\n",
      "[4000]\tvalid_0's l1: 859042\n",
      "[4050]\tvalid_0's l1: 858993\n",
      "[4100]\tvalid_0's l1: 859193\n",
      "[4150]\tvalid_0's l1: 859353\n",
      "[4200]\tvalid_0's l1: 859145\n",
      "[4250]\tvalid_0's l1: 859080\n",
      "[4300]\tvalid_0's l1: 859041\n",
      "[4350]\tvalid_0's l1: 858854\n",
      "[4400]\tvalid_0's l1: 858736\n",
      "[4450]\tvalid_0's l1: 858721\n",
      "[4500]\tvalid_0's l1: 858930\n",
      "[4550]\tvalid_0's l1: 858860\n",
      "[4600]\tvalid_0's l1: 858663\n",
      "[4650]\tvalid_0's l1: 858813\n",
      "[4700]\tvalid_0's l1: 858520\n",
      "[4750]\tvalid_0's l1: 858274\n",
      "[4800]\tvalid_0's l1: 858082\n",
      "[4850]\tvalid_0's l1: 858353\n",
      "[4900]\tvalid_0's l1: 858349\n",
      "[4950]\tvalid_0's l1: 858324\n",
      "[5000]\tvalid_0's l1: 858612\n",
      "[5050]\tvalid_0's l1: 858040\n",
      "[5100]\tvalid_0's l1: 857865\n",
      "[5150]\tvalid_0's l1: 857671\n",
      "[5200]\tvalid_0's l1: 857492\n",
      "[5250]\tvalid_0's l1: 857573\n",
      "[5300]\tvalid_0's l1: 857368\n",
      "[5350]\tvalid_0's l1: 857555\n",
      "[5400]\tvalid_0's l1: 857532\n",
      "[5450]\tvalid_0's l1: 857332\n",
      "[5500]\tvalid_0's l1: 856969\n",
      "[5550]\tvalid_0's l1: 856931\n",
      "[5600]\tvalid_0's l1: 856863\n",
      "[5650]\tvalid_0's l1: 857047\n",
      "[5700]\tvalid_0's l1: 856972\n",
      "[5750]\tvalid_0's l1: 856724\n",
      "[5800]\tvalid_0's l1: 856772\n",
      "[5850]\tvalid_0's l1: 856835\n",
      "[5900]\tvalid_0's l1: 856887\n",
      "[5950]\tvalid_0's l1: 856695\n",
      "[6000]\tvalid_0's l1: 856626\n",
      "[6050]\tvalid_0's l1: 856568\n",
      "[6100]\tvalid_0's l1: 856603\n",
      "[6150]\tvalid_0's l1: 856718\n",
      "[6200]\tvalid_0's l1: 856616\n",
      "[6250]\tvalid_0's l1: 856488\n",
      "[6300]\tvalid_0's l1: 856349\n",
      "[6350]\tvalid_0's l1: 856520\n",
      "[6400]\tvalid_0's l1: 856667\n",
      "[6450]\tvalid_0's l1: 856715\n",
      "[6500]\tvalid_0's l1: 856853\n",
      "[6550]\tvalid_0's l1: 857041\n",
      "[6600]\tvalid_0's l1: 857059\n",
      "[6650]\tvalid_0's l1: 857152\n",
      "[6700]\tvalid_0's l1: 857251\n",
      "[6750]\tvalid_0's l1: 857346\n",
      "[6800]\tvalid_0's l1: 857593\n",
      "[6850]\tvalid_0's l1: 857585\n",
      "[6900]\tvalid_0's l1: 857617\n",
      "[6950]\tvalid_0's l1: 857568\n",
      "[7000]\tvalid_0's l1: 857421\n",
      "[7050]\tvalid_0's l1: 857435\n",
      "[7100]\tvalid_0's l1: 857261\n",
      "[7150]\tvalid_0's l1: 857166\n",
      "[7200]\tvalid_0's l1: 856983\n",
      "[7250]\tvalid_0's l1: 856993\n",
      "[7300]\tvalid_0's l1: 857083\n",
      "[7350]\tvalid_0's l1: 857321\n",
      "[7400]\tvalid_0's l1: 857453\n",
      "[7450]\tvalid_0's l1: 857717\n",
      "[7500]\tvalid_0's l1: 857850\n",
      "[7550]\tvalid_0's l1: 857709\n",
      "[7600]\tvalid_0's l1: 857911\n",
      "[7650]\tvalid_0's l1: 858047\n",
      "[7700]\tvalid_0's l1: 857922\n",
      "[7750]\tvalid_0's l1: 857785\n",
      "[7800]\tvalid_0's l1: 857717\n",
      "[7850]\tvalid_0's l1: 857613\n",
      "[7900]\tvalid_0's l1: 857584\n",
      "[7950]\tvalid_0's l1: 857664\n",
      "[8000]\tvalid_0's l1: 857661\n",
      "[8050]\tvalid_0's l1: 857747\n",
      "[8100]\tvalid_0's l1: 857949\n",
      "[8150]\tvalid_0's l1: 858163\n",
      "[8200]\tvalid_0's l1: 858251\n",
      "[8250]\tvalid_0's l1: 858371\n",
      "[8300]\tvalid_0's l1: 858257\n",
      "[8350]\tvalid_0's l1: 857974\n",
      "[8400]\tvalid_0's l1: 857911\n",
      "[8450]\tvalid_0's l1: 857976\n",
      "[8500]\tvalid_0's l1: 858171\n",
      "[8550]\tvalid_0's l1: 858093\n",
      "[8600]\tvalid_0's l1: 858186\n",
      "[8650]\tvalid_0's l1: 858156\n",
      "[8700]\tvalid_0's l1: 858219\n",
      "[8750]\tvalid_0's l1: 858294\n",
      "[8800]\tvalid_0's l1: 858388\n",
      "[8850]\tvalid_0's l1: 858368\n",
      "[8900]\tvalid_0's l1: 858420\n",
      "[8950]\tvalid_0's l1: 858513\n",
      "[9000]\tvalid_0's l1: 858481\n",
      "[9050]\tvalid_0's l1: 858495\n",
      "[9100]\tvalid_0's l1: 858454\n",
      "[9150]\tvalid_0's l1: 858370\n",
      "[9200]\tvalid_0's l1: 858316\n",
      "[9250]\tvalid_0's l1: 858390\n",
      "[9300]\tvalid_0's l1: 858424\n",
      "[9350]\tvalid_0's l1: 858461\n",
      "[9400]\tvalid_0's l1: 858505\n",
      "[9450]\tvalid_0's l1: 858575\n",
      "[9500]\tvalid_0's l1: 858591\n",
      "[9550]\tvalid_0's l1: 858643\n",
      "[9600]\tvalid_0's l1: 858788\n",
      "[9650]\tvalid_0's l1: 858824\n",
      "[9700]\tvalid_0's l1: 858890\n",
      "[9750]\tvalid_0's l1: 858989\n",
      "[9800]\tvalid_0's l1: 858967\n",
      "[9850]\tvalid_0's l1: 858927\n",
      "[9900]\tvalid_0's l1: 858806\n",
      "[9950]\tvalid_0's l1: 858846\n",
      "[10000]\tvalid_0's l1: 858855\n"
     ]
    }
   ],
   "source": [
    "d_train = lgb.Dataset(X_train.values, label=Y_train.values)\n",
    "d_valid = lgb.Dataset(X_val.values, label=Y_val.values)\n",
    "watchlist = [d_valid]\n",
    "reg = lgb.train(params, d_train, n_estimators, valid_sets=watchlist, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos las predicciones\n",
    "pred_dangerous = reg.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b['precio'] = pred_dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = test_b.reset_index()[['id', 'precio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>precio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4941</td>\n",
       "      <td>7.231153e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>262957</td>\n",
       "      <td>6.182953e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>253578</td>\n",
       "      <td>7.178361e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62134</td>\n",
       "      <td>7.759135e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>277353</td>\n",
       "      <td>5.869489e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        precio\n",
       "0    4941  7.231153e+06\n",
       "1  262957  6.182953e+06\n",
       "2  253578  7.178361e+06\n",
       "3   62134  7.759135e+06\n",
       "4  277353  5.869489e+06"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tenemos test b\n",
    "test_b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52355"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_a['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7645"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_b['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_a.append(test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test['id'].values\n",
    "valores = test['precio'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "escribir_respuesta(ids, valores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
