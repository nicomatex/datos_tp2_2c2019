{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from feature_generation.ipynb\n",
      "Importing Jupyter notebook from pre_processing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import feature_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_generation.get_features()\n",
    "features_types = feature_generation.get_features_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_appended_dataset():\n",
    "    \"\"\"Carga ambos dataframes (train y test) y devuelve un solo dataframe que es el resultado de realizar un\n",
    "    append entre ambos. El dataset de test esta al final.\"\"\"\n",
    "\n",
    "    train = pd.read_csv('./data/train.csv', index_col='id', parse_dates=['fecha'])\n",
    "    test = pd.read_csv('./data/test.csv', index_col='id', parse_dates=['fecha'])\n",
    "    df = train.append(test, sort=False)\n",
    "    df = df[['titulo', 'descripcion', 'direccion', 'tipodepropiedad', 'ciudad', 'provincia', 'antiguedad',\n",
    "         'habitaciones', 'garages', 'banos', 'metroscubiertos', 'metrostotales', 'idzona', 'fecha',\n",
    "         'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos', 'precio']]\n",
    "    \n",
    "    whitelist_tipodepropiedad = df['tipodepropiedad'].value_counts().index.to_list()[:9]\n",
    "    df['tipodepropiedad'] = df['tipodepropiedad'].apply(lambda x: x if x in whitelist_tipodepropiedad else 'Otro') \n",
    "    \n",
    "    #whitelist_ciudad = df['ciudad'].value_counts().index.to_list()[:80]\n",
    "    #df['ciudad'] = df['ciudad'].apply(lambda x: x if x in whitelist_ciudad else 'Otro')\n",
    "    \n",
    "    categoricas = ['tipodepropiedad', 'ciudad', 'provincia']\n",
    "    df[categoricas] = df[categoricas].fillna('unknown')\n",
    "    df[categoricas] = df[categoricas].astype('category')\n",
    "    enteras = ['gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos']\n",
    "    df[enteras] = df[enteras].astype('int8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_test(df):\n",
    "    \"\"\"Separa df compuesto por sus primeras 240.000 filas de train.csv y sus 60.000 ultimas filas de test\n",
    "    en los dos dataframes correspondientes.\"\"\"\n",
    "    \n",
    "    train = df.head(240000)\n",
    "    test = df.tail(60000).drop('precio', axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Carga ambos dataframes (train y test) y devuelve dos datasets en el formato:\n",
    "    train_df, test_df\"\"\"\n",
    "    \n",
    "    df = load_appended_dataset()\n",
    "    return separate_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_featured_appended_dataset():\n",
    "    \"\"\"Carga el .csv con los features generados, y lo formatea para ahorrar memoria. Devuelve un unico\n",
    "    dataframe cuyas primeras 240.000 filas son de train.csv y sus ultimas 60.000 de test.\"\"\"\n",
    "    features_types = feature_generation.get_features_types()\n",
    "    \n",
    "    df = pd.read_csv('./data/full_featured.csv', index_col='id')\n",
    "    df[features_types['bernoulli']] = df[features_types['bernoulli']].astype('uint8')\n",
    "    df[features_types['float']] = df[features_types['float']].astype('float')\n",
    "    df[features_types['uint8']] = df[features_types['uint8']].astype('uint8')\n",
    "    df[features_types['uint16']] = df[features_types['uint16']].astype('uint16')\n",
    "    df[features_types['uint32']] = df[features_types['uint32']].astype('uint32')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_featured_datasets():\n",
    "    \"\"\"Devuelve dos datasets en el formato:\n",
    "    train_df, test_df\n",
    "    Para los datasets con las features generadas.\"\"\"\n",
    "    \n",
    "    df = load_featured_appended_dataset()\n",
    "    return separate_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre procesamiento de datasets externos\n",
    "### Cotizacion USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd = pd.read_csv('data/usd_mxn_diario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funciones aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aniomes(x):\n",
    "    anio=x[6:]\n",
    "    mes=x[3:5]\n",
    "    return anio+mes\n",
    "\n",
    "def numeric(x):\n",
    "    entero, fraccion = x.split(',')\n",
    "    return int(entero) + int(fraccion)/10000\n",
    "\n",
    "def varianza(x):\n",
    "    x = x.rstrip('%')\n",
    "    entero, fraccion = x.split(',')\n",
    "    num = entero+\".\"+fraccion\n",
    "    return float(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd['aniomes'] = usd['Fecha'].map(aniomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = ['Último', 'Apertura', 'Máximo', 'Mínimo']\n",
    "for valor in valores:\n",
    "    usd[valor] = usd[valor].map(numeric)\n",
    "    \n",
    "usd['usd_varianza'] = usd['% var.'].map(varianza)\n",
    "usd['daily_mean'] = usd.apply(lambda x: (x['Último'] + x['Apertura'])/2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd = usd[['aniomes', 'usd_varianza', 'daily_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "usd = usd.groupby('aniomes')['usd_varianza', 'daily_mean'].agg({'usd_varianza':'sum', 'daily_mean':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usd_varianza</th>\n",
       "      <th>daily_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aniomes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>201201</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>13.419316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201202</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>12.789329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201203</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>12.734559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201204</td>\n",
       "      <td>1.67</td>\n",
       "      <td>13.047955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201205</td>\n",
       "      <td>9.99</td>\n",
       "      <td>13.630126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         usd_varianza  daily_mean\n",
       "aniomes                          \n",
       "201201          -6.71   13.419316\n",
       "201202          -1.50   12.789329\n",
       "201203          -0.27   12.734559\n",
       "201204           1.67   13.047955\n",
       "201205           9.99   13.630126"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd.to_csv('data/usd_mxn_featured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
